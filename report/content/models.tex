\section{Models}
\subsection{madmom}
The madmom \cite{bock2016a} library is a framework that provides different deep learning models for music retrieval tasks.
The RNN based beat tracking model \cite{bock2016b} uses multiple mel-spectrogram representations as input to a bidirectional LSTM \cite{hochreiter1997}, postprocessed by a dynamic Bayesian Network (DBN).

The raw audio is split into overlapping frames with a \(10ms\) hop size. Each frame is weighted by a Hann window before computing the Short-time Fourier Transform (STFT). The magnitude of the STFT produces the spectrogram. Three spectrograms are computed using window sizes of 1024, 2048, and 4096 samples. The frequency ranges are limited to \([30, 17000]\) Hz. Logarithmically spaced filters with 3, 6, and 12 bands per octave (corresponding to the three window sizes) are applied to reduce the frequency dimension to a total of 157 bands. The filter outputs are combined with their first order differences, to capture temporal information, resulting in 314 features.

The features are interpreted by a bidirectional Long Short Term Memory (BLSTM). Each layer contains a forward and a backward LSTM, that process the sequence in opposite directions. The network has 3 layers, each with output size of \(25 \times 2\). The final layer consists of a softmax activated linear layer of size 3, predicting the \textit{beat} \(b_k\), \textit{downbeat} \(d_k\), and \textit{non-beat} \(n_k\) classes at each frame \(k\). It consists of around 100k parameters.


The DBN processes the frame predictions, to compute the most likely beat positions. It assigns a discrete state \(s_k\) to every frame, with hidden parameters \textit{bar position} (frames since last downbeat), \textit{tempo} (60 possible BPM logarithmically spaced values), and \textit{time signature} (locked at 4/4). Every frame, the bar position is incremented, until a downbeat position is reached. The tempo and time signature determine the possible downbeat positions \(\mathcal{D}\) and beat positions \(\mathcal{B}\). Tempo transitions can only occur on beat positions and prefer smooth tempo changes over sudden jumps. The observation likelihood \(p(o_k | s_k)\) represents how well the BLSTM output \(o_k = (b_k, d_k, n_k)\) matches the state \(s_k\)
\[ p(o_k | s_k) = \begin{cases} d_k &\text{ if } s_k \in \mathcal{D}\\ b_k &\text{ if } s_k \in \mathcal{B}\\ \frac{n_k}{15} &\text{ else} \end{cases} \]
The DBN assigns a state to every frame, such that the likelihood of the total sequence \([1:K]\) is maximized, considering the transitions and activations from the BLSTM.
\[ s^*_{1:K} = \arg \max_{s_{1:K}} p(s_{1:K} | o_{1:K}) \]
The Viterbi algorithm finds the states that best fit the network predictions and transitions. The final sequence of beats and downbeats, are given by the frames that were assigned to beat-downbeat states.
\[ \mathbf{B} = \{k: s^*_k \in \mathcal{B}\} \qquad\qquad \mathbf{D} = \{k: s^*_k \in \mathcal{D}\} \]
Finally the chosen positions are shifted to the highest model activations within a small window.

\subsection{Beat This}
The beat this \cite{foscarin2024} model, leverages transformers \cite{vaswani2017} to achieve state of the art performance. The authors argue that the DBN relies on rigid musical assumptions, that do not apply to every song. Beat this replaces the DBN with minimal postprocessing.

The preprocessing takes 30 second segments of the audio sampled at 22.05 kHz and converts them into a 128-band log-mel spectrogram limited to \([30, 10000]\) Hz. The window size of 1024 and hop size of 441 samples, create 50 frames per second. The final shape consists of \textit{batch size} \(B\), \textit{time frames} \(T\), 128 \textit{frequency bands} \(F\) and one \textit{channel} \(C\).

The model uses a CNN to compress the frequency bands and increase the channels with a \(3 \times 4\) kernel to a size of \(B \times T \times 32 \times 32\). Three frontend blocks extract features from the frequency information, followed by a concatenation of the channels and a linear layer to 512 features. Six Transformer blocks then process the sequence. Finally the Task Heads apply two sigmoid activated linear layers, for beats \(b_t\) and downbeats \(d_t\). Since downbeats and beat predictions could appear on different positions, the downbeats are added to the pre-activated beats \(b_t = \sigma (\tilde{b}_t + \tilde{d}_t)\).

The \textbf{Frontend Block} adapts the Band-Split RoFormer\cite{lu2024}, utilizing two transformers along different axes, with RoPE\cite{su2021} and a head size of \(32\). The frequency-directed transformer treats each time frame as a sequence of frequency bands \((B \times T) \times F \times C\). The time-directed transformer treats each frequency band as a sequence of time frames \((B \times F) \times T \times C\). Finally a CNN doubles the number of channels \(C\) and halves the frequency bands \(F\), with a freq-stride of 2.

The \textbf{Transformer Block} contains 16 heads of size 32 and a hidden size in the feedforward network of 4 times the features, resulting in 2048 units.

The authors adjust the standard Binary Cross Entropy loss to weight positive examples stronger. To counter the fact, that the model learns "blurred" peaks, max pooling with a 7 frames size is used to consider only the largest prediction 3 frames away.