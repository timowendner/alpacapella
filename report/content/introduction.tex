\section{Introduction}
A beat is the basic rhythmic pulse in music. In rap, this pulse is crucial because rap vocals are rhythmic speech delivered over an instrumental, where rhythm takes precedence over melody. Beats are organized in groups called measures, where the first beat in each measure is called the downbeat. Most research focuses on diverse music genres, but rap and acapellas remains underrepresented.

Accurate beat tracking is vital for music information retrieval (MIR). Downstream tasks like automatic remixing need to align the beats and measures to prevent clashing rhythms. For rap acapella guided music generation\cite{joshi2025rap} it has been shown that accurate beat information could improve performance.

Initially, this task seems simple because most rap instrumentals use looped samples and drum machines, which keep the tempo, measured in beats per minute (BPM), constant.

However, several factors make beat tracking for rap acapellas significantly harder than for standard music. Traditional models use onsets (sharp increases in volume), which are missing from vocals. The timing of rap vocals varies drastically, as they might lag behind, rush ahead, use triplets or off-beat patterns. The recording quality differs widely, from heavily processed tracks with compression artifacts to clean recordings.

Despite advances in beat tracking, existing models have not been evaluated on rap acapellas. This work addresses this gap in research by testing two state-of-the-art models. Beat This\cite{foscarin2024beatthis} combines CNNs with transformers to capture both local and global temporal information. Madmom\cite{boeck2016madmom} leverage RNNs for beat inference.

Evaluation uses three metrics that reveal different aspects of performance. F1 scores measure whether individual beats are detected accurately. CMLt and AMLt assess whether the model locks onto the correct tempo and maintains it consistently.

The main contributions of this paper are:
\begin{enumerate}
    \item The introduction of a new dataset consisting of 110 manually annotated rap acapellas from Looperman\cite{looperman}.
    \item An evaluation baseline for existing models on rap acapellas.
    \item An analysis of limiting factors, such as annotation lag and downbeat ambiguity.
\end{enumerate}