\begin{abstract}
\begin{center}
\begin{minipage}{0.7\textwidth}
\noindent Beat tracking for rap acapellas is a challenging task in Music Information Retrieval (MIR) due to the lack of clear onsets and the complex rhythmic nature of vocal performances. This study addresses the research gap in this domain by evaluating state-of-the-art beat tracking models, on a newly created dataset, counting 110 manually annotated rap acapellas.
An annotation pipeline was developed to ensure data quality by combining multiple annotations. Evaluation results indicate that the RNN-based madmom library outperforms the transformer-based Beat This model. While both models achieve reasonable performance for beats, downbeat detection remains more challenging.
This work provides a new evaluation baseline and dataset to facilitate future research in vocal-based beat tracking.
\end{minipage}
\end{center}
\end{abstract}